# -*- coding: utf-8 -*-
"""SNG_482 and SNG_528_.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16V6lVmu1ySVj32kFuSKrevjl5ONC8S91
"""

import pandas as pd
import json
from copy import deepcopy
from datetime import datetime,date,timedelta
import hashlib
import pycountry
from numpyencoder import NumpyEncoder
from os.path import exists
import boto3
import requests
import os




#NOTE: Object for output json file
out_list = []
total_profile_available = 0

#NOTE: Filename according to the date :
today_date  = date.today()
yesterday = today_date - timedelta(days = 1)
last_updated_string = datetime.now().strftime("%Y-%m-%dT%H:%M:%S")

dag_name = "usa-ofig"

input_filename  = f'{dag_name}-inout-{today_date.day}-{today_date.month}-{today_date.year}.csv'
output_filename = f'{dag_name}-output-{today_date.day}-{today_date.month}-{today_date.year}.json'
diffrance_filename = f'{dag_name}-diffrance-{today_date.day}-{today_date.month}-{today_date.year}.json'
removed_filename = f'{dag_name}-removed-{today_date.day}-{today_date.month}-{today_date.year}.json'
old_output_filename = f'{dag_name}-output-{yesterday.day}-{yesterday.month}-{yesterday.year}.json'
lp_name = f'{dag_name}-logfile.csv'
#NOTE: Paths of directories
root = "/home/ubuntu/sanctions-scripts/USA-OFIG/"
# root = ""
ip_path = f"{root}inputfiles"
op_path = f"{root}outputfiles"
dp_path = f"{root}diffrancefiles"
rm_path = f"{root}removedfiles"
lp_path = f"{root}{dag_name}-logfile.csv"


last_updated_string = datetime.now().strftime("%Y-%m-%dT%H:%M:%S")
ind_schema = {
        "uid": "",
        "name": "",
        "alias_name": [],
        "country": [],
        "list_type": "Individual",
        "last_updated": last_updated_string,
        "list_id": "USA_E20079",
        "individual_details": {
            "date_of_birth": [],
            "gender": "",
        },
        "designation":[],
        "nns_status": "False",
        "address": [
            {
                "street":"",
                "city":"",
                "ZIP_code":"",
                "country": "",
                "complete_address": ""
            }
        ],
        "sanction_Details": {
            "Prohibited Practice":""
        },
        "documents": {
            "passport": [],
            "national_id":[],
            "SSN": ""
        },
        "comment": "",
        "sanction_list": {
            "sl_authority": "The Office of Inspector General, U.S. Department of Health & Human Services",
            "sl_url": "https://oig.hhs.gov/exclusions/exclusions_list.asp",
            "watch_list": "North America Watchlists",
            "sl_host_country": "USA",
            "sl_type": "Sanctions",
            "sl_source": "Office of Inspector General, U.S. Department of Health & Human Services(US) List of Excluded Individuals and Entities, USA",
            "sl_description": "List of the Excluded Individuals and Entities by Office of Inspector General, U.S. Department of Health & Human Services",

            }
    }
ent_schema = {
        "uid": "",
        "name": "",
        "alias_name": [],
        "country": [],
        "list_type": "Entity",
        "entity_details": {},
        "last_updated": last_updated_string,
        "list_id": "USA_E20079",
        "nns_status": "False",
        "address": [
            {
                "street":"",
                "city":"",
                "ZIP_code":"",
                "country": "",
                "complete_address": ""
            }
        ],
        "sanction_Details": {},
        "documents": {},
        "comment": "",
        "sanction_list": {
            "sl_authority": "The Office of Inspector General, U.S. Department of Health & Human Services",
            "sl_url": "https://oig.hhs.gov/exclusions/exclusions_list.asp",
            "watch_list": "North America Watchlists",
            "sl_host_country": "USA",
            "sl_type": "Sanctions",
            "sl_source": "Office of Inspector General, U.S. Department of Health & Human Services(US) List of Excluded Individuals and Entities, USA",
            "sl_description": "List of the Excluded Individuals and Entities by Office of Inspector General, U.S. Department of Health & Human Services",

            }
}

import hashlib
def get_hash(n):
    return hashlib.sha256(((n+ent_schema['sanction_list']['sl_host_country']+ ent_schema['sanction_list']['sl_source']).lower()).encode()).hexdigest()

def alias_name(name):
    alias_list = []
    subname = name.split(' ')
    l = len(subname)
    if l >= 3:
        name1 = subname[l-1] + " " + subname[0]
        name2 = subname[l-2] + " " + subname[0]
        alias_list.append(name1)
        alias_list.append(name2)
    if l == 2:
        name1 = subname[1] + " " + subname[0]
        alias_list.append(name1)

    return alias_list

import re
# t = '1) abc d1234efg567'
def re_operations(string):
    new = re.sub(r'[0-9)\n]+', '', string)
    return new.strip()
# print(re_operations(t))
def countryname(code):
    try:
        pyc = pycountry.countries.get(alpha_2=code)

        return pyc.name
    except:
        return code

# df = pd.read_excel('/content/new_DF2.xlsx')

def sourcedownloder():
    url = "https://oig.hhs.gov/exclusions/downloadables/UPDATED.csv"
    response = requests.get(url)
    try:
        with open(f'{ip_path}/{input_filename}', 'wb') as file:
            file.write(response.content)
    except FileNotFoundError:
        os.mkdir(ip_path)
        with open(f'{ip_path}/{input_filename}', 'wb') as file:
            file.write(response.content)


"""#*SNG-528*"""

def process_data():
    global out_list,last_updated_string,total_profile_available
    df = pd.read_csv(f'{ip_path}/{input_filename}')
    df.shape[0]

    df2=pd.to_datetime(df['DOB'], format='%Y%m%d')
    type(df2[75163].strftime('%Y-%m-%d'))

    df.iloc[14303]

    # for i in df.columns:
    row=df.iloc[-1]
    # print(row)
    # print(dd.strftime('%Y-%m-%d'))
    # dateformat = lambda x:datetime.fromtimestamp(x).strftime('%Y-%m-%d')
    # dateformat(dd)
    # cmpt_addr = ",".join([str(row['ADDRESS']),str(row['ZIP']),str(row['CITY']),str(row['STATE']),str(row['Country'])]).replace(',nan','').replace('nan','').replace(',0,',',')
    # cmpt_addr

    name = " ".join([str(row['LASTNAME']),str(row['FIRSTNAME']),str(row['MIDNAME'])]).strip().replace(',nan','').replace('nan','').replace('  ',' ')
    name

    final_list_SNG528 = []

    for i in range(df.shape[0]):
        where = ""
        row = df.iloc[i]
        # print(i)
        # cmpt_addr = ",".join([str(row['ADDRESS']),str(row['ZIP']),str(row['CITY']),str(row['STATE']),str(row['Country'])]).replace(',nan','').replace('nan','').replace(',0,',',')
        cmpt_addr = ",".join([str(row['ADDRESS']),str(row['ZIP']),str(row['CITY']),str(row['STATE'])]).replace(',nan','').replace('nan','').replace(',0,',',')
        if type(row['BUSNAME'])==str:
            sheet = ent_schema
            where = "entity"
        else:
            sheet = ind_schema
            where = "individual"
        if where == "entity":
            name = row['BUSNAME']
            ent_schema['name'] = name
            # ent_schema['alias_name'] = alias_name(name)
            ent_schema['uid'] = get_hash(name)
            ent_schema['address'][0]['complete_address'] = cmpt_addr
            ent_schema['address'][0]['street'] = str(row['ADDRESS']) if str(row['ADDRESS'])!="nan" else ""
            ent_schema['address'][0]['city'] = str(row['CITY']) if str(row['CITY'])!="nan" else ""
            ent_schema['address'][0]['ZIP_code'] = str(row['ZIP'])
            # try:
            #     geoloc = geolocator.geocode(f"{row['CITY']} {row['ZIP']}")
            #     ent_schema['address'][0]['country'] = geoloc.address.split(",")[-1].strip()
            #     ent_schema["country"].append(geoloc.address.split(",")[-1].strip())
            # except:
            #     ent_schema['address'][0]['country'] = ""

            if 'unique physician identification number' in ent_schema['entity_details']:
                del(ent_schema['entity_details']['unique physician identification number'])
            
            if 'National Provider Identifier' in ent_schema['entity_details']:
                del(ent_schema['entity_details']['National Provider Identifier'])

            if type(row['UPIN'])==str:
                ent_schema['entity_details']['unique physician identification number'] = [row['UPIN']]
            
            if row['NPI']!=0:
                ent_schema['entity_details']['National Provider Identifier'] = [row['NPI']]
            cc = deepcopy(ent_schema)
            final_list_SNG528.append(cc)
            
        else:
            name = " ".join([str(row['FIRSTNAME']),str(row['MIDNAME']),str(row['LASTNAME'])]).strip().replace(',nan','').replace('nan','').replace('  ',' ')
            aliasfromname = " ".join([str(row['LASTNAME']),str(row['FIRSTNAME']),str(row['MIDNAME'])]).strip().replace(',nan','').replace('nan','').replace('  ',' ')
            ind_schema['name'] = name
            ind_schema['alias_name'] = alias_name(name)
            ind_schema['alias_name'].append(aliasfromname)
            ind_schema['uid'] = get_hash(name)
            ind_schema['address'][0]['complete_address'] = cmpt_addr
            ind_schema['address'][0]['street'] = str(row['ADDRESS']) if str(row['ADDRESS'])!="nan" else ""
            ind_schema['address'][0]['city'] = str(row['CITY']) if str(row['CITY'])!="nan" else ""
            ind_schema['address'][0]['ZIP_code'] = str(row['ZIP'])
            # try:
            #     geoloc = geolocator.geocode(f"{row['CITY']} {row['ZIP']}")
            #     ind_schema['address'][0]['country'] = geoloc.address.split(",")[-1].strip()
            #     ind_schema["country"].append(geoloc.address.split(",")[-1].strip())
            # except:
            #     ind_schema['address'][0]['country'] = ""
            
            if 'unique physician identification number' in ind_schema['individual_details']:
                del(ind_schema['individual_details']['unique physician identification number'])
            
            if 'National Provider Identifier' in ind_schema['individual_details']:
                del(ind_schema['individual_details']['National Provider Identifier'])

            if type(row['UPIN'])==str:
                ind_schema['individual_details']['unique physician identification number'] = [row['UPIN']]
            
            if row['NPI']!=0:
                ind_schema['individual_details']['National Provider Identifier'] = [row['NPI']]
            ind_schema['individual_details']['date_of_birth'] = []
            if type(df2[i]) == pd._libs.tslibs.timestamps.Timestamp:
                ind_schema['individual_details']['date_of_birth'] = [df2[i].strftime('%Y-%m-%d')]
            cc = deepcopy(ind_schema)
            final_list_SNG528.append(cc)

    # print(len(final_list_SNG528),final_list_SNG528[-1])
    out_list = deepcopy(final_list_SNG528)
    total_profile_available = len(out_list)
    print(f"Total profile available: {total_profile_available}")
    try:
        with open(f'{op_path}/{output_filename}', "w",encoding='utf-8') as outfile:
            json.dump(out_list, outfile, ensure_ascii=False, indent=4, separators=(', ', ': '),cls=NumpyEncoder)
    except FileNotFoundError:
        os.mkdir(op_path)
        with open(f'{op_path}/{output_filename}', "w",encoding='utf-8') as outfile:
            json.dump(out_list, outfile, ensure_ascii=False, indent=4, separators=(', ', ': '),cls=NumpyEncoder)

def CompareDocument():
    try:
        with open(f'{op_path}/{old_output_filename}', 'rb') as f:
            data = f.read()
    except:
        print("---------------------Alert--------------------------")
        print(f"There is not any old file for date: {yesterday.ctime()}")
        print("----------------------------------------------------")
        data = "No DATA"
    old_list = []
    if data != "No DATA":   
        old_list = json.loads(data)

    with open(f'{op_path}/{output_filename}', 'rb') as f:
        new_data = f.read()
    new_list =  json.loads(new_data)

    new_profiles = []
    removed_profiles = []
    updated_profiles = []

    old_dict = {}
    if old_list:
        for val in old_list:
            old_dict[val["uid"]] = val
        
    new_uid_list = []
    for val1 in new_list:
        new_uid = val1["uid"]
        new_uid_list.append(new_uid)
        if new_uid in old_dict.keys():
            # print("Already in List")
            for i in val1:
                if i!= "last_updated":
                    try:
                        if val1[i] != old_dict[val1["uid"]][i]:
                            # print(f"Updataion Detected on: {val1['uid']} for: {i}")
                            # print(f"Updataion Detected for: {val1[i]}")
                            updated_profiles.append(val1)
                            break
                    except:
                        # print(f"Updataion Detected on: {val1['uid']} for: {i}")
                        # print(f"Updataion Detected for: {val1[i]}")
                        updated_profiles.append(val1)
                        break

        else:
            new_profiles.append(val1)
            # print(f"New Profile Detected : {val1['uid']}")
    

    for val2 in old_dict.keys():
        if val2 not in new_uid_list:
            # print(f"Removed Profile Detected : {val2}")
            removed_profiles.append(old_dict[val2])

    print("------------------------LOG-DATA---------------------------")
    print(f"total New Profiles Detected:     {len(new_profiles)}")
    print(f"total Updated Profiles Detected: {len(updated_profiles)}")
    print(f"total Removed Profiles Detected: {len(removed_profiles)}")
    print("-----------------------------------------------------------")

    if len(new_list)==0:
        removed_profiles = []
        raise ValueError("Error : Data Parsing Error.... fix it quick ‚öíÔ∏è")
    try:
        with open(f'{dp_path}/{diffrance_filename}', "w",encoding='utf-8') as outfile:
            json.dump(new_profiles+updated_profiles, outfile,ensure_ascii=False)
    except FileNotFoundError:
        os.mkdir(dp_path)
        with open(f'{dp_path}/{diffrance_filename}', "w",encoding='utf-8') as outfile:
            json.dump(new_profiles+updated_profiles, outfile,ensure_ascii=False)

    try:
        with open(f'{rm_path}/{removed_filename}', "w",encoding='utf-8') as outfile:
            json.dump(removed_profiles, outfile,ensure_ascii=False)
    except FileNotFoundError:
        os.mkdir(rm_path)
        with open(f'{rm_path}/{removed_filename}', "w",encoding='utf-8') as outfile:
            json.dump(removed_profiles, outfile,ensure_ascii=False)

    if exists(lp_path):
        with open(f'{lp_path}',"a") as outfile:
            passing = f"{last_updated_string},{input_filename},{output_filename},{total_profile_available},{len(new_list)},{len(new_profiles)},{len(updated_profiles)},{len(new_profiles)+len(updated_profiles)},{len(removed_profiles)},{diffrance_filename},{removed_filename}\n"
            outfile.write(passing)
    else:
        with open(f'{lp_path}',"a") as outfile:
            pass_first = "date,inputfile,outputfile,total_profile_availabe,total_profile_scraped,new,updated,diffrance,removed,diffrancefile,removedfile\n"
            passing = f"{last_updated_string},{input_filename},{output_filename},{total_profile_available},{len(new_list)},{len(new_profiles)},{len(updated_profiles)},{len(new_profiles)+len(updated_profiles)},{len(removed_profiles)},{diffrance_filename},{removed_filename}\n"
            outfile.write(pass_first)
            outfile.write(passing)

def UploadfilestTos3():
    try:
        print("uploading files to s3")
        s3 = boto3.client('s3')
        s3.upload_file(f'{ip_path}/{input_filename}',"sams-scrapping-data",f"{dag_name}/original/{input_filename}")
        s3.upload_file(f'{op_path}/{output_filename}',"sams-scrapping-data",f"{dag_name}/parced/{output_filename}")
        s3.upload_file(f'{dp_path}/{diffrance_filename}',"sams-scrapping-data",f"{dag_name}/diffrance/{diffrance_filename}")
        s3.upload_file(f'{rm_path}/{removed_filename}',"sams-scrapping-data",f"{dag_name}/removed/{removed_filename}")
        s3.upload_file(f'{lp_path}',"sams-scrapping-data",f"{dag_name}/{lp_name}")
        print("uploaded files to s3")      
    except Exception as e:
        print("------------------üî¥ALERTüî¥------------------------")
        print("Can not upload files to s3")
        print("Exception : " , e)
        print("----------------------------------------------------")



sourcedownloder()
process_data()
CompareDocument()
UploadfilestTos3()