# -*- coding: utf-8 -*-
"""French_site.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1mC3IxbAnTsKjKiOA-eNH5S_3hiv7dT1K
"""

from bs4 import BeautifulSoup as bs
import pandas as pd
pd.set_option('display.max_colwidth', 500)
import time
import requests
import random
from datetime import datetime,date,timedelta
import json
import copy
import re
import hashlib
from deep_translator import GoogleTranslator
import boto3
import os
from os.path import exists


#NOTE: Object for output json file
out_list = []
input_list = []
total_profile_available = 0

#NOTE: Filename according to the date :
today_date  = date.today()
yesterday = today_date - timedelta(days = 1)
last_updated_string = datetime.now().strftime("%Y-%m-%dT%H:%M:%S")

dag_name = "french-sanc"

input_filename  = f'{dag_name}-inout-{today_date.day}-{today_date.month}-{today_date.year}.json'
output_filename = f'{dag_name}-output-{today_date.day}-{today_date.month}-{today_date.year}.json'
diffrance_filename = f'{dag_name}-diffrance-{today_date.day}-{today_date.month}-{today_date.year}.json'
removed_filename = f'{dag_name}-removed-{today_date.day}-{today_date.month}-{today_date.year}.json'
old_output_filename = f'{dag_name}-output-{yesterday.day}-{yesterday.month}-{yesterday.year}.json'
lp_name = f'{dag_name}-logfile.csv'
#NOTE: Paths of directories
root = "/home/ubuntu/sanctions-scripts/FRENCH/"
# root = ""
ip_path = f"{root}inputfiles"
op_path = f"{root}outputfiles"
dp_path = f"{root}diffrancefiles"
rm_path = f"{root}removedfiles"
lp_path = f"{root}{dag_name}-logfile.csv"


page= requests.get("https://gels-avoirs.dgtresor.gouv.fr/List")
page

soup=bs(page.content,"lxml")

last_updated_string = datetime.now().strftime("%Y-%m-%dT%H:%M:%S")
ind_schema = {
        "uid": "",
        "name": "",
        "alias_name": [],
        "country": [],
        "list_type": "Individual",
        "last_updated": last_updated_string,
        "list_id": "FRA_E20147",
        "individual_details": {
            "date_of_birth": [],
            "gender": ""
        },
        "designation":[],
        "nns_status": "False",
        "address": [
            {
                "country": "",
                "complete_address": ""
            }
        ],
        "documents": {
            "passport": [],
            "SSN": ""
        },
        "comment": "",
        "sanction_list": {
            "sl_authority": "Directorate General of the Treasury, Ministry of Economy, Finance and Recovery, France",
            "sl_url": "https://gels-avoirs.dgtresor.gouv.fr/List",
            "watch_list": "European Watchlists",
            "sl_host_country": "France",
            "sl_type": "Sanctions",
            # "sl_source": "Ministry of Finance, French Freezing of Assets Sanctions List",
            "sl_source": "Freezing of Assets, Ministry of Finance, France",
            "sl_description": "Sanctions imposed by the Directorate General of the Treasury, Ministry of Economy, Finance and Recovery, France"

        }
    }
ent_schema = {
        "uid": "",
        "name": "",
        "alias_name": [],
        "country": [],
        "list_type": "Entity",
        "last_updated": last_updated_string,
        "list_id": "FRA_E20147",
        "nns_status": "False",
        "address": [
            {
                "country": "",
                "complete_address": ""
            }
        ],
        "documents": {},
        "comment": "",
        "sanction_list": {
            "sl_authority": "Directorate General of the Treasury, Ministry of Economy, Finance and Recovery, France",
            "sl_url": "https://gels-avoirs.dgtresor.gouv.fr/List",
            "watch_list": "European Watchlists",
            "sl_host_country": "France",
            "sl_type": "Sanctions",
            # "sl_source": "Ministry of Finance, French Freezing of Assets Sanctions List",
            "sl_source": "Freezing of Assets, Ministry of Finance, France",
            "sl_description": "Sanctions imposed by the Directorate General of the Treasury, Ministry of Economy, Finance and Recovery, France"

        }
    }


def langtranslation(to_translate):
    try:
        translated = GoogleTranslator(source='auto', target='en').translate(to_translate)
    except:
        try:
            translated = GoogleTranslator(source='auto', target='en').translate(to_translate)
        except:
            print(f">>>Translartion Bug : {to_translate}")
            translated = to_translate   
    return translated

# -*- coding: utf-8 -*-
def isEnglish(s):
    try:
        s.encode(encoding='utf-8').decode('ascii')
    except UnicodeDecodeError:
        return False
    else:
        return True



def get_hash(n):
    return hashlib.sha256(((n + "Ministry of Finance, French Freezing of Assets Sanctions List" + "France").lower()).encode()).hexdigest()
def alias_name(name):
    alias_list = []
    subname = name.split(' ')
    l = len(subname)
    if l >= 3:
        name1 = subname[l-1] + " " + subname[0]
        name2 = subname[l-2] + " " + subname[0]
        alias_list.append(name1)
        alias_list.append(name2)
    if l == 2:
        name1 = subname[1] + " " + subname[0]
        alias_list.append(name1)

    return alias_list


last_updated_string = datetime.now().strftime("%Y-%m-%dT%H:%M:%S")

ID_S=[i.text for i in soup.find_all(class_="tableGelsColumn80")]
total=[i.text for i in soup.find_all("td")]
print("total = ",len(total))
cols=["ID","Diet","nature_type","Last_name","First_name","A.k.a","Date_of_Birth","Place_of_birth","Nationality","Title","Address","Passport","Identification","Legal_basis","Grounds"]
print(f'number of columns = {len(cols)}')

important_keys = []

print(ID_S[-1])
# sep = [i.strip() for i in total[-16:]]
# print(len(sep),sep)

def create_rows():
    rows = []
    start = 0
    end = 16
    
    for i in range(len(total)//16):
        rows.append(total[start:end])
        start += 16
        end += 16
    if rows:
        input_list.extend(rows)
    return rows
# create_rows()
print(list(map(lambda x: x.strip(), create_rows()[0]))[3].replace('\"',''))

order = {j:i for i,j in enumerate(cols)}
for i in order:
    print(f'{i},--------->{order[i]}')

final_list2 = []
zz=[]
def run():
    for i in create_rows():
        new = list(map(lambda x: x.strip(),i))
        if new[2] =='Personne physique':
            ind_schema['name']=str(new[4]+" "+new[3]).replace('\"','').strip()
            if not isEnglish(ind_schema['name']):
                ind_schema['name'] = langtranslation(ind_schema['name'])
            ind_schema["alias_name"]=[]
            ind_schema["alias_name"]=alias_name(ind_schema['name'])
            if new[5]!='':
                ind_schema['alias_name']+=new[5].split('\n\n')
            ind_schema["uid"]=get_hash(ind_schema['name'])
            if new[6]!='':
                ind_schema['individual_details']['date_of_birth']=new[6].replace('((Vers))','').split('\n\n')
            ind_schema['country']=[]
            ind_schema['address'][0]['complete_address']=''
            ind_schema['address'][0]['country']=''
            if new[10]!='':
                ind_schema['address'][0]['complete_address']=ind_schema['address'][0]['complete_address']=new[10].replace('\n\n',' ,')
                # ind_schema['address'][0]['country']=new[8]
            if new[8]!='':
                ind_schema['country']=new[8].split('\n\n')
                if len(ind_schema['country'])==1:
                    ind_schema['address'][0]['country']=new[8]
                
                if ind_schema['address'][0]['complete_address']!='':
                    ind_schema['address'][0]['complete_address']=ind_schema['address'][0]['complete_address']+','+ind_schema['address'][0]['country']
                else:
                    ind_schema['address'][0]['complete_address']=ind_schema['address'][0]['country']
            try:
                del(ind_schema['individual_details']['place_of_birth'])
                del(ind_schema['documents']['National_id'])
            except:
                pass
            if new[7]!='':
                ind_schema['individual_details']['place_of_birth']=new[7].replace('(','').replace(')','').split('\n\n')
            if new[12]!='':
                ind_schema['documents']['National_id']=new[12].split('\n\n')
            ind_schema['documents']['passport']=new[11].split('\n\n')
            ind_schema['designation']=[]
            if new[9]!='':
                ind_schema['designation']=new[9].split('\n\n')

            cc = copy.deepcopy(ind_schema)
            final_list2.append(cc)
        
        elif new[2] =='Personne morale':
            ent_schema['name']=new[4].strip()+" "+new[3].replace('\"','').strip()
            if not isEnglish(ent_schema['name']):
                ent_schema['name'] = langtranslation(ent_schema['name'])
            ent_schema["alias_name"]=[]
            ent_schema["alias_name"]=alias_name(ent_schema['name'])
            if new[5]!='':
                ent_schema['alias_name']+=new[5].split('\n\n')
            ent_schema["uid"]=get_hash(ent_schema['name'])
            ent_schema['country']=[]
            ent_schema['address'][0]['complete_address']=''
            if new[10]!='':
                ent_schema['address'][0]['complete_address']=ent_schema['address'][0]['complete_address']=new[10].replace('\n\n',' ,')
            if new[8]!='':
                ent_schema['country']=new[8].split('\n\n')
                if len(ent_schema['country'])==1:
                    ent_schema['address'][0]['country']=new[8]
                if ent_schema['address'][0]['complete_address']!='':
                    ent_schema['address'][0]['complete_address']=ent_schema['address'][0]['complete_address']+','+ent_schema['address'][0]['country']
                else:
                    ent_schema['address'][0]['complete_address']=ent_schema['address'][0]['country']
            # ent_schema['documents']['passport']=new[11]
            cc = copy.deepcopy(ent_schema)
            final_list2.append(cc)
            zz.append(new[0])

    global total_profile_available,out_list
    total_profile_available = len(final_list2)
    out_list = copy.copy(final_list2)
    print(f"Total Available Profiles : {total_profile_available}")
    try:
        with open(f'{ip_path}/{input_filename}', "w",encoding='utf-8') as infile:
            json.dump(input_list, infile,ensure_ascii=False)
    except FileNotFoundError:
        os.mkdir(ip_path)
        with open(f'{ip_path}/{input_filename}', "w",encoding='utf-8') as infile:
            json.dump(input_list, infile,ensure_ascii=False)       
    try:
        with open(f'{op_path}/{output_filename}', "w",encoding='utf-8') as outfile:
            json.dump(out_list, outfile,ensure_ascii=False)
    except FileNotFoundError:
        os.mkdir(op_path)
        with open(f'{op_path}/{output_filename}', "w",encoding='utf-8') as outfile:
            json.dump(out_list, outfile,ensure_ascii=False)       

def CompareDocument():
    try:
        with open(f'{op_path}/{old_output_filename}', 'rb') as f:
            data = f.read()
    except:
        print("---------------------Alert--------------------------")
        print(f"There is not any old file for date: {yesterday.ctime()}")
        print("----------------------------------------------------")
        data = "No DATA"
    old_list = []
    if data != "No DATA":   
        old_list = json.loads(data)
    new_list = copy.deepcopy(out_list)

    new_profiles = []
    removed_profiles = []
    updated_profiles = []

    old_dict = {}
    if old_list:
        for val in old_list:
            old_dict[val["uid"]] = val
        
    new_uid_list = []
    for val1 in new_list:
        new_uid = val1["uid"]
        new_uid_list.append(new_uid)
        if new_uid in old_dict.keys():
            # print("Already in List")
            for i in val1:
                if i!= "last_updated":
                    try:
                        if val1[i] != old_dict[val1["uid"]][i]:
                            print(f"Updataion Detected on: {val1['uid']} for: {i}")
                            # print(f"Updataion Detected for: {val1[i]}")
                            updated_profiles.append(val1)
                            break
                    except:
                        print(f"Updataion Detected on: {val1['uid']} for: {i}")
                        # print(f"Updataion Detected for: {val1[i]}")
                        updated_profiles.append(val1)
                        break

        else:
            new_profiles.append(val1)
            print(f"New Profile Detected : {val1['uid']}")
    

    for val2 in old_dict.keys():
        if val2 not in new_uid_list:
            print(f"Removed Profile Detected : {val2}")
            removed_profiles.append(old_dict[val2])

    print("------------------------LOG-DATA---------------------------")
    print(f"total New Profiles Detected:     {len(new_profiles)}")
    print(f"total Updated Profiles Detected: {len(updated_profiles)}")
    print(f"total Removed Profiles Detected: {len(removed_profiles)}")
    print("-----------------------------------------------------------")
    if len(new_list)==0:
        removed_profiles = []
        raise ValueError("Error : Data Parsing Error.... fix it quick ⚒️")
    try:
        with open(f'{dp_path}/{diffrance_filename}', "w",encoding='utf-8') as outfile:
            json.dump(new_profiles+updated_profiles, outfile,ensure_ascii=False)
    except FileNotFoundError:
        os.mkdir(dp_path)
        with open(f'{dp_path}/{diffrance_filename}', "w",encoding='utf-8') as outfile:
            json.dump(new_profiles+updated_profiles, outfile,ensure_ascii=False)

    try:
        with open(f'{rm_path}/{removed_filename}', "w",encoding='utf-8') as outfile:
            json.dump(removed_profiles, outfile,ensure_ascii=False)
    except FileNotFoundError:
        os.mkdir(rm_path)
        with open(f'{rm_path}/{removed_filename}', "w",encoding='utf-8') as outfile:
            json.dump(removed_profiles, outfile,ensure_ascii=False)

    if exists(lp_path):
        with open(f'{lp_path}',"a") as outfile:
            passing = f"{today_date.ctime()},{input_filename},{output_filename},{total_profile_available},{len(new_list)},{len(new_profiles)},{len(updated_profiles)},{len(new_profiles)+len(updated_profiles)},{len(removed_profiles)},{diffrance_filename},{removed_filename}\n"
            outfile.write(passing)
    else:
        with open(f'{lp_path}',"a") as outfile:
            pass_first = "date,inputfile,outputfile,total_profile_availabe,total_profile_scraped,new,updated,diffrance,removed,diffrancefile,removedfile\n"
            passing = f"{today_date.ctime()},{input_filename},{output_filename},{total_profile_available},{len(new_list)},{len(new_profiles)},{len(updated_profiles)},{len(new_profiles)+len(updated_profiles)},{len(removed_profiles)},{diffrance_filename},{removed_filename}\n"
            outfile.write(pass_first)
            outfile.write(passing)

#NOTE : Method to upload files in s3 bucket (Bhavesh Chavda)
def UploadfilestTos3():
    try:
        print("uploading files to s3")
        s3 = boto3.client('s3')
        s3.upload_file(f'{ip_path}/{input_filename}',"sams-scrapping-data",f"{dag_name}/original/{input_filename}")
        s3.upload_file(f'{op_path}/{output_filename}',"sams-scrapping-data",f"{dag_name}/parced/{output_filename}")
        s3.upload_file(f'{dp_path}/{diffrance_filename}',"sams-scrapping-data",f"{dag_name}/diffrance/{diffrance_filename}")
        s3.upload_file(f'{rm_path}/{removed_filename}',"sams-scrapping-data",f"{dag_name}/removed/{removed_filename}")
        s3.upload_file(f'{lp_path}',"sams-scrapping-data",f"{dag_name}/{lp_name}")
        print("uploaded files to s3")      
    except Exception as e:
        print("------------------🔴ALERT🔴------------------------")
        print("Can not upload files to s3")
        print("Exception : " , e)
        print("----------------------------------------------------")




run()
CompareDocument()
UploadfilestTos3()

