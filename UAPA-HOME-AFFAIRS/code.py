# -*- coding: utf-8 -*-
"""script_Ministry of Home Affairs_SNG-481.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1dwIikufidDCbhZpxKi-kgerISnXwedmY
"""

import hashlib
from bs4 import BeautifulSoup as bs
import pandas as pd
import time
import requests
import random
from datetime import datetime,date,timedelta
import json
from copy import deepcopy
import re
import boto3
import os
from os.path import exists
import copy


#NOTE: Object for output json file
out_list = []
input_list = []
total_profile_available = 0

#NOTE: Filename according to the date :
today_date  = date.today()
yesterday = today_date - timedelta(days = 1)
last_updated_string = datetime.now().strftime("%Y-%m-%dT%H:%M:%S")

dag_name = "UAPA-Home-affairs"

input_filename  = f'{dag_name}-inout-{today_date.day}-{today_date.month}-{today_date.year}.json'
output_filename = f'{dag_name}-output-{today_date.day}-{today_date.month}-{today_date.year}.json'
diffrance_filename = f'{dag_name}-diffrance-{today_date.day}-{today_date.month}-{today_date.year}.json'
removed_filename = f'{dag_name}-removed-{today_date.day}-{today_date.month}-{today_date.year}.json'
old_output_filename = f'{dag_name}-output-{yesterday.day}-{yesterday.month}-{yesterday.year}.json'
lp_name = f'{dag_name}-logfile.csv'
#NOTE: Paths of directories
root = "/home/ubuntu/sanctions-scripts/UAPA-HOME-AFFAIRS/"
# root = ""
ip_path = f"{root}inputfiles"
op_path = f"{root}outputfiles"
dp_path = f"{root}diffrancefiles"
rm_path = f"{root}removedfiles"
lp_path = f"{root}{dag_name}-logfile.csv"



page = """

 """

page = requests.get("https://www.mha.gov.in/individual-terrorists-under-uapa")
page

soup = bs(page.content,features="lxml")


data = [i.text.strip() for i in soup.find_all('p')]
# print(len(data), data)
all = [i.split('@') for i in data]
aliases = [i[1:] for i in all]
aliases


last_updated_string = datetime.now().strftime("%Y-%m-%dT%H:%M:%S")
ind_schema = {
    "uid": "",
    "name": "",
    "alias_name": [],
    "country": [],
    "list_type": "Individual",
    "last_updated": last_updated_string,
    "list_id": "IND_E20007",
    "individual_details": {
            "date_of_birth": [],
            "gender": ""
    },
    "designation": [],
    "nns_status": "False",
    "address": [
        {
            "street": "",
            "city": "",
            "ZIP_code": "",
            "country": "",
            "complete_address": ""
        }
    ],
    "sanction_Details": {},
    "documents": {
        "passport": [],
        "national_id": [],
        "SSN": ""
    },
    "comment": "",
    "sanction_list": {
        "sl_authority": "Ministry of Home Affairs",
        "sl_url": "https://www.mha.gov.in/individual-terrorists-under-uapa",
        "watch_list": "India Watchlists",
        "sl_host_country": "India",
        "sl_type": "Sanctions",
        "sl_source": "Ministry of Home Affairs Individual Terrorists under UAPA",
        "sl_description": "List of Terrorists by Ministry of Home Affairs Individual Terrorists under UAPA",

    }
}


def get_hash(n):
    return hashlib.sha256(((n+ind_schema['sanction_list']['sl_host_country'] + ind_schema['sanction_list']['sl_source']).lower()).encode()).hexdigest()


def alias_name(name):
    alias_list = []
    subname = name.split(' ')
    l = len(subname)
    if l >= 3:
        name1 = subname[l-1] + " " + subname[0]
        name2 = subname[l-2] + " " + subname[0]
        aa = " ".join(subname[:-1])
        name3 = subname[-1] + " " + aa
        alias_list.append(name3)
        alias_list.append(name1)
        alias_list.append(name2)
    if l == 2:
        name1 = subname[1] + " " + subname[0]
        alias_list.append(name1)

    return alias_list


def process_data():
    final_list = []
    for i in all:
        # print(i)
        if '.' in i[0]:
            name = i[0][i[0].index('.')+1:]
            if ',' in name:
                name = name.split(',')[0]
        else:
            name = i[0][3:]
            if ',' in name:
                name = name.split(',')[0]
        aliases = i[1:]
        ind_schema['name'] = name.strip()
        ind_schema['uid'] = get_hash(name)
        ind_schema['alias_name'] = []
        ind_schema['alias_name'] = aliases
        ind_schema['alias_name'] += alias_name(name)
        ind_schema['alias_name'] = [i.strip() for i in ind_schema['alias_name'] if i.strip()]
  
        cc = deepcopy(ind_schema)
        final_list.append(cc)
        # print(name)
    # print(len(final_list), final_list[-1])
    global total_profile_available,out_list
    total_profile_available = len(final_list)
    out_list = copy.deepcopy(final_list)
    print(f"Total Available Profiles : {total_profile_available}")
    #NOTE: There is not any Input FIle to store.
    # try:
    #     with open(f'{ip_path}/{input_filename}', "w",encoding='utf-8') as infile:
    #         json.dump(input_list, infile,ensure_ascii=False,indent=2)
    # except FileNotFoundError:
    #     os.mkdir(ip_path)
    #     with open(f'{ip_path}/{input_filename}', "w",encoding='utf-8') as infile:
    #         json.dump(input_list, infile,ensure_ascii=False,indent=2)       
    try:
        with open(f'{op_path}/{output_filename}', "w",encoding='utf-8') as outfile:
            json.dump(out_list, outfile,ensure_ascii=False,indent=2)
    except FileNotFoundError:
        os.mkdir(op_path)
        with open(f'{op_path}/{output_filename}', "w",encoding='utf-8') as outfile:
                json.dump(out_list, outfile,ensure_ascii=False,indent=2)       

def CompareDocument():
    try:
        with open(f'{op_path}/{old_output_filename}', 'rb') as f:
            data = f.read()
    except:
        print("---------------------Alert--------------------------")
        print(f"There is not any old file for date: {yesterday.ctime()}")
        print("----------------------------------------------------")
        data = "No DATA"
    old_list = []
    if data != "No DATA":   
        old_list = json.loads(data)
    new_list = copy.deepcopy(out_list)

    new_profiles = []
    removed_profiles = []
    updated_profiles = []

    old_dict = {}
    if old_list:
        for val in old_list:
            old_dict[val["uid"]] = val
        
    new_uid_list = []
    for val1 in new_list:
        new_uid = val1["uid"]
        new_uid_list.append(new_uid)
        if new_uid in old_dict.keys():
            # print("Already in List")
            for i in val1:
                if i!= "last_updated":
                    try:
                        if val1[i] != old_dict[val1["uid"]][i]:
                            print(f"Updataion Detected on: {val1['uid']} for: {i}")
                            # print(f"Updataion Detected for: {val1[i]}")
                            updated_profiles.append(val1)
                            break
                    except:
                        print(f"Updataion Detected on: {val1['uid']} for: {i}")
                        # print(f"Updataion Detected for: {val1[i]}")
                        updated_profiles.append(val1)
                        break

        else:
            new_profiles.append(val1)
            print(f"New Profile Detected : {val1['uid']}")
    

    for val2 in old_dict.keys():
        if val2 not in new_uid_list:
            print(f"Removed Profile Detected : {val2}")
            removed_profiles.append(old_dict[val2])

    # if len(removed_profiles) == len(new_list) and len(new_profiles)+len(updated_profiles)==0:
    #     removed_profiles = []
    if len(new_list)==0:
        removed_profiles = []
        raise ValueError("Error : Data Parsing Error.... fix it quick ‚öíÔ∏è")

     
    print("------------------------LOG-DATA---------------------------")
    print(f"total New Profiles Detected:     {len(new_profiles)}")
    print(f"total Updated Profiles Detected: {len(updated_profiles)}")
    print(f"total Removed Profiles Detected: {len(removed_profiles)}")
    print("-----------------------------------------------------------")

    try:
        with open(f'{dp_path}/{diffrance_filename}', "w",encoding='utf-8') as outfile:
            json.dump(new_profiles+updated_profiles, outfile,ensure_ascii=False)
    except FileNotFoundError:
        os.mkdir(dp_path)
        with open(f'{dp_path}/{diffrance_filename}', "w",encoding='utf-8') as outfile:
            json.dump(new_profiles+updated_profiles, outfile,ensure_ascii=False)

    try:
        with open(f'{rm_path}/{removed_filename}', "w",encoding='utf-8') as outfile:
            json.dump(removed_profiles, outfile,ensure_ascii=False)
    except FileNotFoundError:
        os.mkdir(rm_path)
        with open(f'{rm_path}/{removed_filename}', "w",encoding='utf-8') as outfile:
            json.dump(removed_profiles, outfile,ensure_ascii=False)

    if exists(lp_path):
        with open(f'{lp_path}',"a") as outfile:
            passing = f"{last_updated_string},{output_filename},{total_profile_available},{len(new_list)},{len(new_profiles)},{len(updated_profiles)},{len(new_profiles)+len(updated_profiles)},{len(removed_profiles)},{diffrance_filename},{removed_filename}\n"
            outfile.write(passing)
    else:
        with open(f'{lp_path}',"a") as outfile:
            pass_first = "date,outputfile,total_profile_availabe,total_profile_scraped,new,updated,diffrance,removed,diffrancefile,removedfile\n"
            passing = f"{last_updated_string},{output_filename},{total_profile_available},{len(new_list)},{len(new_profiles)},{len(updated_profiles)},{len(new_profiles)+len(updated_profiles)},{len(removed_profiles)},{diffrance_filename},{removed_filename}\n"
            outfile.write(pass_first)
            outfile.write(passing)


def UploadfilestTos3():
    try:
        print("uploading files to s3")
        s3 = boto3.client('s3')
        # s3.upload_file(f'{ip_path}/{input_filename}',"sams-scrapping-data",f"{dag_name}/original/{input_filename}")
        s3.upload_file(f'{op_path}/{output_filename}',"sams-scrapping-data",f"{dag_name}/parced/{output_filename}")
        s3.upload_file(f'{dp_path}/{diffrance_filename}',"sams-scrapping-data",f"{dag_name}/diffrance/{diffrance_filename}")
        s3.upload_file(f'{rm_path}/{removed_filename}',"sams-scrapping-data",f"{dag_name}/removed/{removed_filename}")
        s3.upload_file(f'{lp_path}',"sams-scrapping-data",f"{dag_name}/{lp_name}")
        print("uploaded files to s3")      
    except Exception as e:
        print("------------------üî¥ALERTüî¥------------------------")
        print("Can not upload files to s3")
        print("Exception : " , e)
        print("----------------------------------------------------")



process_data()
CompareDocument()
UploadfilestTos3()